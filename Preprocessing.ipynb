{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tarizatique/notebooka7a8d01ae0?scriptVersionId=105909505\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#importing all importing libraries\n\n%matplotlib inline\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#importing data\ndatan= pd.read_csv('/content/drive/your dataset path/filename.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data Inspection**","metadata":{}},{"cell_type":"code","source":"data.head()  #it will show the first 5 rows of the dataset you can define manually the numbers of row you want to see\ndata.tail()  #it will show the last 5 rows of the dataset\ndata.shape   # it will show the number of rows and columns are present in the dataset\ndata.info()  #it will show the data types and also dataset have null values or not\ndata.describe() #it will show statistical view of the data mean,median, standard deviation....","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For stats","metadata":{}},{"cell_type":"code","source":"data.mean()\ndata.median()\ndata.mode()\ndata.std()\ndata.var()\ndata.skew()\ndata.kurt()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**First we will check missing values**","metadata":{}},{"cell_type":"code","source":"data.isnull().sum()\ndata.isna().sum()\n# function for couting missing values\n# Missing Value Count Function\ndef show_missing():\n    missing = data_train.columns[data_train.isnull().any()].tolist()\n    return missing\n\n# Missing data counts and percentage\nprint('Missing Data Count')\nprint(data_train[show_missing()].isnull().sum().sort_values(ascending = False))\nprint('--'*50)\nprint('Missing Data Percentage')\nprint(round(data_train[show_missing()].isnull().sum().sort_values(ascending = False)/len(data_train)*100,2))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Handling the Missing values**","metadata":{}},{"cell_type":"code","source":"# There are many ways to handle missing value like dropping the missing values or imputing with mean, median and mode\n# to drop all missing values\ndata.dropna()\n#you can also drop the columns after checking the percentage of each column\n# Features with over 50% of its observations missings will be removed\ndata_train = data_train.drop(['PoolQC','MiscFeature','Alley','Fence'],axis = 1) # you can put the columns names here it's just for example\n#imputing the missing values\n#you can fill missing values by using fillna and can put any value\ndata.fillna(0)\n# It will the Na value with previous value of the column\ndf.fillna(method ='pad')\n#it will with next value\ndf.fillna(method ='bfill')\n\n#filling na value by using Simple Imputer\nfrom sklearn.impute import SimpleImputer\n \n# Imputer object using the mean strategy and\n# missing_values type for imputation\nimputer = SimpleImputer(missing_values = np.nan,\n                        strategy ='mean') #  you can either do it with mean , median or most frequently\n\n# Fitting the data to the imputer object\nimputer = imputer.fit(data)\n \n# Imputing the data    \ndata = imputer.transform(data)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Detecting Outliers ","metadata":{}},{"cell_type":"code","source":"# Detection of outliers \n#you can either outliers by using data visualisation like Boxplot, histogram ....\nIQR = df['CRIM'].quantile(0.75) - df['CRIM'].quantile(0.25)\nlower_limit = df['CRIM'].quantile(0.25) - (IQR * 1.5)\nupper_limit = df['CRIM'].quantile(0.75) + (IQR * 1.5)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Treating Outliers","metadata":{}},{"cell_type":"code","source":"# We can treat outliers by 3 method removing it,replacing or by winsorisation\n############### 1. Remove (let's trim the dataset) ################\n# Trimming Technique\n# let's flag the outliers in the data set\noutliers_df = np.where(df['CRIM'] > upper_limit, True, np.where(df['CRIM'] < lower_limit, True, False))\ndf_trimmed = df.loc[~(outliers_df), ]\ndf.shape, df_trimmed.shape\n\n# let's explore outliers in the trimmed dataset\nsns.boxplot(df_trimmed.CRIM)\n# we see no outiers\n\n############### 2.Replace ###############\n# Now let's replace the outliers by the maximum and minimum limit\ndf['df_replaced'] = pd.DataFrame(np.where(df['Salaries'] > upper_limit, upper_limit, np.where(df['Salaries'] < lower_limit, lower_limit, df['Salaries'])))\nsns.boxplot(df.df_replaced)\n\n############### 3. Winsorization ###############\n# pip install feature_engine   # install the package\nfrom feature_engine.outliers import Winsorizer\nwinsor = Winsorizer(capping_method='iqr', # choose  IQR rule boundaries or gaussian for mean and std\n                          tail='both', # cap left, right or both tails \n                          fold=1.5,\n                          variables=['Salaries'])\n\ndf_t = winsor.fit_transform(df[['Salaries']])\n\n# we can inspect the minimum caps and maximum caps \n# winsor.left_tail_caps_, winsor.right_tail_caps_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Discretisation and binning","metadata":{}},{"cell_type":"code","source":"# it is use to convert numerical into categorical data \ndata['Salaries_new'] = pd.cut(data['Salaries'], bins=[min(data.Salaries) - 1, \n                                                  data.Salaries.mean(), max(data.Salaries)], labels=[\"Low\",\"High\"])\n\n#binning method\nbins = [0, 5, 7,10]\npd.cut(data['OverallQual'], bins).value_counts()\n\ndata['OverallQual_binned'] = pd.cut(data_train['OverallQual'], bins)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dummy variable and Label Encoding","metadata":{}},{"cell_type":"code","source":"#dummy variable,one hot encoding and label encoding used to convert categorical data into numerical\n#for nominal data we use dummy variable and for ordinal we use label encoder\n# Create dummy variables\ndf_new = pd.get_dummies(df)\ndf_new_1 = pd.get_dummies(df, drop_first = True)\n# we have created dummies for all categorical columns\n\nfrom sklearn.preprocessing import OneHotEncoder\n# Creating instance of One Hot Encoder\nenc = OneHotEncoder() # initializing method\n\nenc_df = pd.DataFrame(enc.fit_transform(df.iloc[:, 2:]).toarray())\n\n#######################\n# Label Encoder\n\nfrom sklearn.preprocessing import LabelEncoder\n# creating instance of labelencoder\nlabelencoder = LabelEncoder()\n#for single column\nX['Sex']= labelencoder.fit_transform(X['Sex'])\n### label encode y ###\ny = labelencoder.fit_transform(y)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"standardisation and normalisatio","metadata":{}},{"cell_type":"code","source":"### Standardization\nfrom sklearn.preprocessing import StandardScaler\n# Initialise the Scaler\nscaler = StandardScaler()\n# To scale data\ndata = scaler.fit_transform(data)\n\n### Normalization function - Custom Function\n# Range converts to: 0 to 1\ndef norm_func(i):\n    x = (i-i.min())/(i.max()-i.min())\n    return(x)\n\ndata = norm_func(data)\n\n#you can either use MinMaxScaler\n# import module\nfrom sklearn.preprocessing import MinMaxScaler\n\n# scale features\nscaler = MinMaxScaler()\nmodel=scaler.fit(data)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Type Casting","metadata":{}},{"cell_type":"code","source":"#use to convert values from one data type another data type\n# Now we will convert 'float64' into 'int64' type. \ndata.Salaries = data.Salaries.astype('int64')\ndata.dtypes\n#int into float\ndata.age = data.age.astype('float32')\ndata.dtypes\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Duplicate handling","metadata":{}},{"cell_type":"code","source":"### Identify duplicates records in the data ###\nduplicate = data.duplicated()\nduplicate\nsum(duplicate)\n\n# Removing Duplicates\ndata1 = data.drop_duplicates()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Log Transformation**\nLogarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering.\n\nWhat are the benefits of log transform:\n\nIt helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n\nIn most of the cases the magnitude order of the data changes within the range of the data.\n\nIt also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust.\n","metadata":{}},{"cell_type":"code","source":"data['natural_log'] = np.log(data['Salary'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}